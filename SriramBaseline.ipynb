{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 FINAL Project - San Francisco Crime Classification\n",
    "\n",
    "Project Team: Shih Yu Chang, Sriram Rao, Jingjing Rong, Frank Xie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Predict the category of crimes that occurred in the city by the bay\n",
    "\n",
    "Background: From 1934 to 1963, San Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of Alcatraz.\n",
    "\n",
    "Today, the city is known more for its tech scene than its criminal past. But, with rising wealth inequality, housing shortages, and a proliferation of expensive digital toys riding BART to work, there is no scarcity of crime in the city by the bay.\n",
    "\n",
    "From Sunset to SOMA, and Marina to Excelsior, this competition's dataset provides nearly 12 years of crime reports from across all of San Francisco's neighborhoods. Given time and location, you must predict the category of crime that occurred.\n",
    "This is a baseline Model that assigns random labels to the dev dataset\n",
    "\n",
    "Dataset: The dataset contains incidents derived from SFPD Crime Incident Reporting system. The data ranges from 1/1/2003 to 5/13/2015. The training set and test set rotate every week, meaning week 1,3,5,7... belong to test set, week 2,4,6,8 belong to training set. \n",
    "\n",
    "Data fields:\n",
    "    Dates - timestamp of the crime incident\n",
    "    Category - category of the crime incident (only in train.csv). This is the target variable you are going to predict.\n",
    "    Descript - detailed description of the crime incident (only in train.csv)\n",
    "    DayOfWeek - the day of the week\n",
    "    PdDistrict - name of the Police Department District\n",
    "    Resolution - how the crime incident was resolved (only in train.csv)\n",
    "    Address - the approximate street address of the crime incident \n",
    "    X - Longitude\n",
    "    Y - Latitude\n",
    "\n",
    "Evaluation: Submissions are evaluated using the multi-class logarithmic loss. Each incident has been labeled with one true class. For each incident, you must submit a set of predicted probabilities (one for every class).\n",
    "\n",
    "Submission Format:You must submit a csv file with the incident id, all candidate class names, and a probability for each class. The order of the rows does not matter\n",
    "\n",
    "Source: https://www.kaggle.com/c/sf-crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "README: This is the baseline submission of the project with the basic processing pipeline and a baseline prediction model. As a baseline, we assume that all crimes as the most common type of crime theft. The baseline solution has a log loss score of 32.89184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler,FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from copy import deepcopy\n",
    "import pygeohash as pgh         # Module to encode Latitude and Longitude as floating point number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data shape  (878049, 9)\n",
      "\n",
      "Printing a few rows                  Dates        Category                        Descript  \\\n",
      "0  2015-05-13 23:53:00        WARRANTS                  WARRANT ARREST   \n",
      "1  2015-05-13 23:53:00  OTHER OFFENSES        TRAFFIC VIOLATION ARREST   \n",
      "2  2015-05-13 23:33:00  OTHER OFFENSES        TRAFFIC VIOLATION ARREST   \n",
      "3  2015-05-13 23:30:00   LARCENY/THEFT    GRAND THEFT FROM LOCKED AUTO   \n",
      "4  2015-05-13 23:30:00   LARCENY/THEFT    GRAND THEFT FROM LOCKED AUTO   \n",
      "5  2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM UNLOCKED AUTO   \n",
      "\n",
      "   DayOfWeek PdDistrict      Resolution                    Address  \\\n",
      "0  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
      "1  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
      "2  Wednesday   NORTHERN  ARREST, BOOKED  VANNESS AV / GREENWICH ST   \n",
      "3  Wednesday   NORTHERN            NONE   1500 Block of LOMBARD ST   \n",
      "4  Wednesday       PARK            NONE  100 Block of BRODERICK ST   \n",
      "5  Wednesday  INGLESIDE            NONE        0 Block of TEDDY AV   \n",
      "\n",
      "            X          Y  \n",
      "0 -122.425892  37.774599  \n",
      "1 -122.425892  37.774599  \n",
      "2 -122.424363  37.800414  \n",
      "3 -122.426995  37.800873  \n",
      "4 -122.438738  37.771541  \n",
      "5 -122.403252  37.713431  \n",
      "\n",
      "List of Unique Crime Categories  ['WARRANTS' 'OTHER OFFENSES' 'LARCENY/THEFT' 'VEHICLE THEFT' 'VANDALISM'\n",
      " 'NON-CRIMINAL' 'ROBBERY' 'ASSAULT' 'WEAPON LAWS' 'BURGLARY'\n",
      " 'SUSPICIOUS OCC' 'DRUNKENNESS' 'FORGERY/COUNTERFEITING' 'DRUG/NARCOTIC'\n",
      " 'STOLEN PROPERTY' 'SECONDARY CODES' 'TRESPASS' 'MISSING PERSON' 'FRAUD'\n",
      " 'KIDNAPPING' 'RUNAWAY' 'DRIVING UNDER THE INFLUENCE'\n",
      " 'SEX OFFENSES FORCIBLE' 'PROSTITUTION' 'DISORDERLY CONDUCT' 'ARSON'\n",
      " 'FAMILY OFFENSES' 'LIQUOR LAWS' 'BRIBERY' 'EMBEZZLEMENT' 'SUICIDE'\n",
      " 'LOITERING' 'SEX OFFENSES NON FORCIBLE' 'EXTORTION' 'GAMBLING'\n",
      " 'BAD CHECKS' 'TREA' 'RECOVERED VEHICLE' 'PORNOGRAPHY/OBSCENE MAT']\n",
      "\n",
      "Number of unique latitude and longitudes 34243\n",
      "\n",
      "Number of unique latitude and longitudes 34243\n"
     ]
    }
   ],
   "source": [
    "# This cell is to load and explore the training data (train.csv file) provided by the Kaggle website\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "print (\"Training Data shape \", train_data.shape)\n",
    "print(\"\\nPrinting a few rows\", train_data.head(6))\n",
    "\n",
    "print(\"\\nList of Unique Crime Categories \", train_data.Category.unique())\n",
    "print(\"\\nNumber of unique latitude and longitudes\", len(train_data.Y.unique()))\n",
    "print(\"\\nNumber of unique latitude and longitudes\", len(train_data.X.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### Data Pre-Processing Section #########\n",
    "# Remove 67 data points with incorrect latitude and longitude (those of Antartica)\n",
    "# Extract Year, Date, Hour from timeline provided in the dataset\n",
    "# Encode the geographical location provided as two values - latitude and longitude as a single floating point number\n",
    "\n",
    "def geoencode(x):\n",
    "    return pgh.encode(x['Latitude'], x['Longitude'], precision=7)\n",
    "\n",
    "\n",
    "# Remove 67 outlier data points that have wrong Latitude and Longitude\n",
    "train_data = train_data[abs(train_data[\"Y\"])<38]\n",
    "\n",
    "# Drop columns that add no value to our analysis\n",
    "train_data = train_data.drop(['Resolution', 'Descript'], axis=1)\n",
    "\n",
    "#Extract Date, Year and Hour from the Dates field\n",
    "train_data['Dates'] = pd.to_datetime(train_data['Dates'])\n",
    "train_data['Year'], train_data['Month']  = train_data['Dates'].dt.year, train_data['Dates'].dt.month \n",
    "train_data['Hour'] = train_data['Dates'].dt.hour\n",
    "\n",
    "#Rename the X, Y columns as longitude and latitude respectively\n",
    "train_data['Latitude'] = train_data['Y']\n",
    "train_data['Longitude'] = train_data['X']\n",
    "train_data = train_data.drop(['X', 'Y'], axis=1)\n",
    "\n",
    "# There are 34243 X 34243 values for lat and long. This may be too granular for the analysis\n",
    "# Therefore we try to geo-encode the location as a floating point number\n",
    "# With a encoding precision of 7, the 34243 X 34243 values get encoded as a matrix with 5000 values \n",
    "# Area of San Francisco is about 47 Square miles - this translates to each sqaure mile sliced in to roughly a 10X10 matrix\n",
    "train_data['GeoCode'] = train_data.apply(geoencode, axis=1)\n",
    "#train_data.to_csv(\"sfcrimedata.csv\")   \n",
    "train_labels = train_data['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There are 800K+ rows of training data. This will bog down the initial analysis.\n",
    "# Create a smaller baseline training and dev data set for quick analysis\n",
    "# Also split the Training data and labels\n",
    "\n",
    "#TO DO: shuffle the data up\n",
    "baseline_train_data_count = 50000\n",
    "btrain_data = train_data[:(baseline_train_data_count*2)]\n",
    "btrain_datacopy = btrain_data\n",
    "btrain_labels = btrain_data['Category']\n",
    "\n",
    "# Get Dummy values of geocode and convert as columns\n",
    "geocode = pd.get_dummies(btrain_data.GeoCode)\n",
    "btrain_data = pd.concat([geocode], axis=1)\n",
    "#btrain_data['Year'] = btrain_datacopy['Year']\n",
    "btrain_data['Month'] = btrain_datacopy['Month']\n",
    "btrain_data['Hour'] = btrain_datacopy['Hour']\n",
    "\n",
    "bdev_data = btrain_data[(baseline_train_data_count+1):]\n",
    "bdev_labels = btrain_labels[(baseline_train_data_count+1):]\n",
    "btrain_data = btrain_data[:baseline_train_data_count]\n",
    "btrain_labels = btrain_labels[:baseline_train_data_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>9q8ys3x</th>\n",
       "      <th>9q8ys6w</th>\n",
       "      <th>9q8ys7x</th>\n",
       "      <th>9q8ys9e</th>\n",
       "      <th>9q8ys9s</th>\n",
       "      <th>9q8ysb8</th>\n",
       "      <th>9q8yscb</th>\n",
       "      <th>9q8yscu</th>\n",
       "      <th>9q8ysf3</th>\n",
       "      <th>9q8ysfd</th>\n",
       "      <th>...</th>\n",
       "      <th>9q8znd7</th>\n",
       "      <th>9q8znd8</th>\n",
       "      <th>9q8znd9</th>\n",
       "      <th>9q8zndb</th>\n",
       "      <th>9q8zndd</th>\n",
       "      <th>9q8zndh</th>\n",
       "      <th>9q8zndj</th>\n",
       "      <th>9q8zndk</th>\n",
       "      <th>9q8zpe0</th>\n",
       "      <th>9q8zpkc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 2614 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   9q8ys3x  9q8ys6w  9q8ys7x  9q8ys9e  9q8ys9s  9q8ysb8  9q8yscb  9q8yscu  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "5        0        0        0        0        0        0        0        0   \n",
       "6        0        0        0        0        0        0        0        0   \n",
       "7        0        0        0        0        0        0        0        0   \n",
       "8        0        0        0        0        0        0        0        0   \n",
       "9        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   9q8ysf3  9q8ysfd   ...     9q8znd7  9q8znd8  9q8znd9  9q8zndb  9q8zndd  \\\n",
       "0        0        0   ...           0        0        0        0        0   \n",
       "1        0        0   ...           0        0        0        0        0   \n",
       "2        0        0   ...           0        0        0        0        0   \n",
       "3        0        0   ...           0        0        0        0        0   \n",
       "4        0        0   ...           0        0        0        0        0   \n",
       "5        0        0   ...           0        0        0        0        0   \n",
       "6        0        0   ...           0        0        0        0        0   \n",
       "7        0        0   ...           0        0        0        0        0   \n",
       "8        0        0   ...           0        0        0        0        0   \n",
       "9        0        0   ...           0        0        0        0        0   \n",
       "\n",
       "   9q8zndh  9q8zndj  9q8zndk  9q8zpe0  9q8zpkc  \n",
       "0        0        0        0        0        0  \n",
       "1        0        0        0        0        0  \n",
       "2        0        0        0        0        0  \n",
       "3        0        0        0        0        0  \n",
       "4        0        0        0        0        0  \n",
       "5        0        0        0        0        0  \n",
       "6        0        0        0        0        0  \n",
       "7        0        0        0        0        0  \n",
       "8        0        0        0        0        0  \n",
       "9        0        0        0        0        0  \n",
       "\n",
       "[10 rows x 2614 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TO DO: Plot a frequency histogram to understand correlation between crime count and each of the following crime type, \n",
    "# day of the week, police district, encoded geo-location, year, month and hour\n",
    "# UPDATE: This was completed in Tableau. \n",
    "#Visualization available in a PDF document - \"W207 - SF Crime Prediction - Data Visualization Dashboard.pdf\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline log loss  33.8694015209\n"
     ]
    }
   ],
   "source": [
    "# Assign a baseline label to dev dataset and check accuracy\n",
    "list1 = [0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "#list1 = [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "baselinepredict = np.array([list1], float)\n",
    "for i in range (2, baseline_train_data_count):\n",
    "    baselinepredict = np.append(baselinepredict, [list1], axis=0)\n",
    "\n",
    "#print(baselinepredict.shape)\n",
    "print(\"Baseline log loss \",log_loss(bdev_labels, baselinepredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest model with 400 trees is: 0.250\n"
     ]
    }
   ],
   "source": [
    "# Since we have too many features, start the analysis with Random Forest\n",
    "Num_Trees = 400\n",
    "rf_model = RandomForestClassifier(n_estimators=Num_Trees, max_depth=50, n_jobs=4)\n",
    "rf_model.fit(btrain_data, btrain_labels)\n",
    "predicted = np.array(rf_model.predict_proba(bdev_data))\n",
    "#print('predicted', predicted)\n",
    "print( 'Accuracy for Random Forest model with %d trees is: %0.3f' %(Num_Trees, rf_model.score(bdev_data, bdev_labels)))\n",
    "#log_loss(bdev_labels, predicted)\n",
    "#print(rf_model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gaussian NaiveBayes\n",
    "#clf = GaussianNB()\n",
    "#clf.fit(btrain_data, btrain_labels)\n",
    "#prediction = clf.predict(bdev_data)\n",
    "#predicted = np.array(clf.predict(bdev_data))\n",
    "#accuracy=clf.score(bdev_data, bdev_labels)\n",
    "#print('Accuracy for GaussianNB is: %0.3f' %(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
