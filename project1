# This tells matplotlib not to try opening a new window for each plot.
%matplotlib inline

# Import a bunch of libraries.
import time
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator
from sklearn.pipeline import Pipeline
from sklearn.datasets import fetch_mldata
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LinearRegression
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import classification_report

# Set the randomizer seed so results are the same each time.
np.random.seed(0)

# Load the digit data either from mldata.org, or once downloaded to data_home, from disk. The data is about 53MB so this cell
# should take a while the first time your run it.
mnist = fetch_mldata('MNIST original', data_home='/datasets/mnist')
X, Y = mnist.data, mnist.target

# Rescale grayscale values to [0,1].
X = X / 255.0

# Shuffle the input: create a random permutation of the integers between 0 and the number of data points and apply this
# permutation to X and Y.
# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.
shuffle = np.random.permutation(np.arange(X.shape[0]))
X, Y = X[shuffle], Y[shuffle]

print('data shape: ', X.shape)
print('label shape:', Y.shape)

# Set some variables to hold test, dev, and training data.
test_data, test_labels = X[61000:], Y[61000:]
dev_data, dev_labels = X[60000:61000], Y[60000:61000]
train_data, train_labels = X[:60000], Y[:60000]
mini_train_data, mini_train_labels = X[:1000], Y[:1000]

################## def P1(num_examples=10): #################
### STUDENT START ###
# set up the plot figure with gray colormap and size of 8 inches X 8 inches
plt.rc('image', cmap='gray')
plt.rc('figure', figsize=[8,8])
fig = plt.figure()

# get a list with indices of data with labels 0 through 9
gridsz = 10
gridsq = gridsz * gridsz

indexdata = np.empty(gridsq)

for i in range(0, gridsz):
    count = 0
    while(count < gridsz):
        r = np.random.randint(0, 60000)
        if ((int(train_labels[r])) == i):
            indexdata[((i*10)+count)]=r
            count=count+1

# plot a 10 X 10 grid with image data to visualize 10 examples of each digit
for i in range(0, gridsq):
    ax = plt.subplot(gridsz, gridsz, i+1) 
    ax.set_axis_off()
    mat = np.reshape(train_data[(indexdata[i])], (28, 28))
    ax.imshow(mat)
    #am.imshow(img)

plt.show()
### STUDENT END ###

#P1(10)

#########################def P2(k_values): ###########################

### STUDENT START ###
#initialize the values of k
kparam=[9,7,5,3,1]

# execute the nearest neighbor classifier
for k in kparam:
    clf = KNeighborsClassifier(k)
    # Train the model with the mini training data set of 1000 records
    clf.fit(mini_train_data, mini_train_labels)
    # Make predictions for the data in dev dataset 
    prediction = clf.predict(dev_data)
    # Compare predicted labels with actual labels
    accuracy=clf.score(dev_data, dev_labels)
    print ("Predicted Nearest Neighbor accuracy for k value of "+str(k)+" is "+str(accuracy))

print("Classification report for classifier %s:\n%s\n"% (clf, classification_report(dev_labels, prediction)))

    
### STUDENT END ###

#k_values = [1, 3, 5, 7, 9]
#P2(k_values)

#####################def P3(train_sizes, accuracies): #####################
### STUDENT START ###
train_sizes = [100, 200, 400, 800, 1600, 3200, 6400, 12800, 25000]
clf = KNeighborsClassifier(1)
accuracies = []

for train_size in train_sizes:
    # Train the model with the mini training data set of 1000 records
    shuffle = np.random.permutation(np.arange(train_data.shape[0]))
    p3_data, p3_label = train_data[shuffle], train_labels[shuffle]

    p3_data, p3_label = p3_data[:train_size], p3_label[:train_size]
    
    starttime = time.time()
    clf.fit(p3_data, p3_label)
    # Make predictions for the data in dev dataset 
    prediction = clf.predict(dev_data)
    endtime = time.time()
    # Compare predicted labels with actual labels
    accuracy=clf.score(dev_data, dev_labels)
    accuracies.append(accuracy)
    print ("Predicted Nearest Neighbor accuracy for a training size of "+str(train_size)+" records is "+str(accuracy))
    print ("Time needed for prediction for a training size of "+str(train_size)+" records is "+str(endtime-starttime)+" seconds")

#print(train_sizes)
#print(accuracies)
### STUDENT END ###

#train_sizes = [100, 200, 400, 800, 1600, 3200, 6400, 12800, 25000]
#accuracies = []
#P3(train_sizes, accuracies)

#def P4(): ############################  PENDING: Transformation ########

### STUDENT START ###
#print(train_sizes)
#print(accuracies)

XX = [[100],[200],[400],[800],[1600],[3200],[6400],[12800],[25000]]
yy = np.array([accuracies])
#YY = np.empty([len(train_sizes),len(train_sizes)] )
#XX = np.empty([3,3])
#YY = np.empty([3,3])
regr = LinearRegression()
regr.fit(XX,accuracies)

X_test = [[60000]]
print(regr.predict(X_test))
# The coefficients
#print('Coefficients: \n', regr.coef_)

logit = LogisticRegression()
logit.fit(XX,accuracies)
print(logit.predict(X_test))

### STUDENT END ###

#P4()

#def P5():  ######  PENDING: Display a few mistakes ########

### STUDENT START ###
clf = KNeighborsClassifier(1)
# Train the model with the mini training data set of 1000 records
clf.fit(mini_train_data, mini_train_labels)
# Make predictions for the data in dev dataset 
prediction = clf.predict(dev_data)
# Compare predicted labels with actual labels
accuracy=clf.score(dev_data, dev_labels)
class1 = [0,1,2,3,4,5,6,7,8,9]
cm = confusion_matrix(prediction, dev_labels, class1)
print(cm)
print ("Predicted Nearest Neighbor accuracy for k value of "+str(k)+" is "+str(accuracy))

num_display = 10
count = 0
k = 0
indexdata1 = np.empty(gridsq)

while ((count < num_display) and (k<len(prediction))):
    if(prediction[k] != dev_labels[k]):
        if(int(prediction[k]) == 4):
            indexdata1[count]=k
            count = count+1
    k = k+1
            
for i in range(0, num_display):
    ax = plt.subplot(1, num_display, i+1) 
    ax.set_axis_off()
    mat = np.reshape(dev_data[(indexdata1[i])], (28, 28))
    ax.imshow(mat)

### STUDENT END ###

#P5()

#def P6():
    
### STUDENT START ###

def funcgaussianblur(datavector):
    datamat = np.reshape(datavector, (28, 28))
    blurdatamat = np.reshape(datavector, (28, 28))
    for rowindex in range(0,27):
        for colindex in range(0,27):
            val = 0
            n = 0
            # Get the values from the previous row
            if (rowindex-1)>=0:
                if(colindex-1)>=0:
                    val = val + datamat[rowindex-1][colindex-1]
                    n=n+1
                val = val + datamat[rowindex-1][colindex]
                n=n+1
                if(colindex+1)<=27:
                    val = val + datamat[rowindex-1][colindex+1]
                    n=n+1
            # Add the values from the current row
            if(colindex-1)>=0:
                val = val + datamat[rowindex][colindex-1]
                n=n+1
            val = val + datamat[rowindex][colindex]
            n=n+1
            if(colindex+1)<=27:
                val = val + datamat[rowindex][colindex+1]
                n=n+1
            # Get the values from the next row
            if (rowindex+1)<=27:
                if(colindex-1)>=0:
                    val = val + datamat[rowindex+1][colindex-1]
                    n=n+1
                val = val + datamat[rowindex+1][colindex]
                n=n+1
                if(colindex+1)<=27:
                    val = val + datamat[rowindex+1][colindex+1]
                    n=n+1
            blurdatamat[rowindex][colindex] = val/n
    blurdatavector = np.reshape(blurdatamat, (1, 784))
    #print(blurdatavector)
    return blurdatavector
  
#preprocess the training data
blurtrain_data = train_data
for i in range(0, train_data.shape[0]):
    blurtrain_data[i] = funcgaussianblur(train_data[i])

clf = KNeighborsClassifier()
clf.fit(blurtrain_data, train_labels)
# Make predictions for the data in dev dataset 
prediction = clf.predict(dev_data)
    
# Compare predicted labels with actual labels
accuracy = clf.score(dev_data, dev_labels)
print ("Predicted Nearest Neighbor accuracy with preprocessing just training data is "+str(accuracy))

#preprocess the dev data
blurdev_data = dev_data
for i in range(0, dev_data.shape[0]):
    blurdev_data[i] = funcgaussianblur(dev_data[i])

clf = KNeighborsClassifier()
clf.fit(train_data, train_labels)
# Make predictions for the data in dev dataset 
prediction = clf.predict(blurdev_data)
    
# Compare predicted labels with actual labels
accuracy = clf.score(blurdev_data, dev_labels)
print ("Predicted Nearest Neighbor accuracy with preprocessing just dev data is "+str(accuracy))

#preprocess both dev and training data
clf = KNeighborsClassifier()
clf.fit(blurtrain_data, train_labels)
# Make predictions for the data in dev dataset 
prediction = clf.predict(blurdev_data)
    
# Compare predicted labels with actual labels
accuracy = clf.score(blurdev_data, dev_labels)
print ("Predicted Nearest Neighbor accuracy with preprocessing both training and dev data is "+str(accuracy))


### STUDENT END ###

#P6()

#def P7():

### STUDENT START : PENDING - Multinomial transformation###

BNBtrain_data = Binarizer(0.1).transform(train_data)
BNBdev_data = Binarizer(0.1).transform(dev_data)
BNBclf = BernoulliNB()
BNBclf.fit(BNBtrain_data, train_labels)
BNBprediction = BNBclf.predict(BNBdev_data)
# Compare predicted labels with actual labels
BNBaccuracy = BNBclf.score(BNBdev_data, dev_labels)
print("Predicted Nearest Neighbor accuracy for BernoulliNB is "+str(BNBaccuracy))

MNBtrain_data = train_data
binplace = np.digitize(MNBtrain_data, bins=[0.0, 0.001, 0.999, 1.0])
MNBtrain_data[np.where(binplace == 1)] = 0
MNBtrain_data[np.where(binplace == 2)] = 1
MNBtrain_data[np.where(binplace == 3)] = 2

MNBdev_data = dev_data
binplace1 = np.digitize(MNBdev_data, bins=[0.0, 0.001, 0.999, 1.0])
MNBdev_data[np.where(binplace1 == 1)] = 0
MNBdev_data[np.where(binplace1 == 2)] = 1
MNBdev_data[np.where(binplace1 == 3)] = 2

        
MNBclf = MultinomialNB()
MNBclf.fit(MNBtrain_data, train_labels)
MNBprediction = MNBclf.predict(MNBtrain_data)
# Compare predicted labels with actual labels
MNBaccuracy = MNBclf.score(MNBtrain_data, train_labels)
print("Predicted Nearest Neighbor accuracy for MultinomialNB is "+str(MNBaccuracy))
#train_data[34347][0]
#MNBtrain_data[34347][0]        
### STUDENT END ###

#P7()

#def P8(alphas): #### What is GridSearchCV - why are we doing this?####

### STUDENT START ###

parameters = [{'alpha':[0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}]

CVclf = GridSearchCV(BernoulliNB(binarize=0.0, class_prior=None, fit_prior=True), parameters, cv=10, scoring='f1_weighted')
CVclf.fit(train_data,train_labels) 

print(CVclf.best_estimator_)
print(CVclf.best_score_)
print(CVclf.best_params_)
print(CVclf.grid_scores_)


### STUDENT END ###

#def P9() ##############################################################:

### STUDENT END ###
GNBclf = GaussianNB()
param = GNBclf.fit(train_data, train_labels)
GNBprediction = GNBclf.predict(dev_data)
# Compare predicted labels with actual labels
GNBaccuracy = GNBclf.score(dev_data, dev_labels)
print("Predicted Nearest Neighbor accuracy for GaussianNB is "+str(GNBaccuracy))
print(param)


### STUDENT END ###

#gnb = P9()

#def P10(num_examples)######################################:

### STUDENT START ###
BNBtrain_data = Binarizer(0.1).transform(train_data)
BNBdev_data = Binarizer(0.1).transform(dev_data)
BNBclf = BernoulliNB()
BNBclf.fit(BNBtrain_data, train_labels)

#print('feature log shape: ', BNBclf.feature_log_prob_.shape[1])
#print('feature log array: ', BNBclf.feature_log_prob_[0][0])

num_features = 784
num_samples = 20
num_digits = 10
gen_data = np.zeros(((num_samples*num_digits), num_features))

for digit in range (0, num_digits):
    for sample in range (0, num_samples):
        for feature in range (0, num_features):
            continuewithfeature = True
            prob = np.exp(BNBclf.feature_log_prob_[digit][feature])
            while(continuewithfeature):
                r = np.random.rand()
                if(r < prob):
                    gen_data[((digit*num_samples)+sample)][feature] = r
                    continuewithfeature = False

        ax = plt.subplot(num_digits, num_samples, (((digit*num_samples)+sample)+1))
        ax.set_axis_off()
        mat = np.reshape(gen_data[((digit*num_samples)+sample)], (28, 28))
        ax.imshow(mat)

### STUDENT END ###

#P10(20)######################################################

#def P11(buckets, correct, total)#############################:
    
### STUDENT START ###
buckets = [0.5, 0.9, 0.999, 0.99999, 0.9999999, 0.999999999, 0.99999999999, 0.9999999999999, 1.0]
correct = [0 for i in buckets]
total = [0 for i in buckets]

#print(total)

BNBclf = BernoulliNB(alpha = 0.9)
BNBclf.fit(BNBtrain_data, train_labels)

for num in range (0, dev_data.shape[0]):
    predicteddigit = int(BNBclf.predict(dev_data[num]))
    posteriorprob = BNBclf.predict_proba(dev_data[num])
    bucketindex = np.digitize(posteriorprob[0][predicteddigit], buckets, right=False)
   
    total[bucketindex-1] = (total[bucketindex-1])+1
    if (dev_labels[num] == predicteddigit):
        correct[bucketindex-1] = (correct[bucketindex-1])+1
    
for i in range(len(buckets)):
    accuracy = 0.0
    if (total[i] > 0): 
        accuracy = correct[i] / total[i]
    print('p(pred) <= %.13f    total = %3d    accuracy = %.3f' %(buckets[i], total[i], accuracy))
                
### STUDENT END ##############################################
