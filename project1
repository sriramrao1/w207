# This tells matplotlib not to try opening a new window for each plot.
%matplotlib inline

# Import a bunch of libraries.
import time
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator
from sklearn.pipeline import Pipeline
from sklearn.datasets import fetch_mldata
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LinearRegression
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import classification_report

# Set the randomizer seed so results are the same each time.
np.random.seed(0)

# Load the digit data either from mldata.org, or once downloaded to data_home, from disk. The data is about 53MB so this cell
# should take a while the first time your run it.
mnist = fetch_mldata('MNIST original', data_home='/datasets/mnist')
X, Y = mnist.data, mnist.target

# Rescale grayscale values to [0,1].
X = X / 255.0

# Shuffle the input: create a random permutation of the integers between 0 and the number of data points and apply this
# permutation to X and Y.
# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.
shuffle = np.random.permutation(np.arange(X.shape[0]))
X, Y = X[shuffle], Y[shuffle]

print('data shape: ', X.shape)
print('label shape:', Y.shape)

# Set some variables to hold test, dev, and training data.
test_data, test_labels = X[61000:], Y[61000:]
dev_data, dev_labels = X[60000:61000], Y[60000:61000]
train_data, train_labels = X[:60000], Y[:60000]
mini_train_data, mini_train_labels = X[:1000], Y[:1000]

################## def P1(num_examples=10): #################
### STUDENT START ###
# set up the plot figure with gray colormap and size of 8 inches X 8 inches
plt.rc('image', cmap='gray')
plt.rc('figure', figsize=[8,8])
fig = plt.figure()

# get a list with indices of data with labels 0 through 9
gridsz = 10
gridsq = gridsz * gridsz

indexdata = np.empty(gridsq)

for i in range(0, gridsz):
    count = 0
    while(count < gridsz):
        r = np.random.randint(0, 60000)
        if ((int(train_labels[r])) == i):
            indexdata[((i*10)+count)]=r
            count=count+1

# plot a 10 X 10 grid with image data to visualize 10 examples of each digit
for i in range(0, gridsq):
    ax = plt.subplot(gridsz, gridsz, i+1) 
    ax.set_axis_off()
    mat = np.reshape(train_data[(indexdata[i])], (28, 28))
    ax.imshow(mat)
    #am.imshow(img)

plt.show()
### STUDENT END ###

#P1(10)

#########################def P2(k_values): ###########################

### STUDENT START ###
#initialize the values of k
kparam=[9,7,5,3,1]

# execute the nearest neighbor classifier
for k in kparam:
    clf = KNeighborsClassifier(k)
    # Train the model with the mini training data set of 1000 records
    clf.fit(mini_train_data, mini_train_labels)
    # Make predictions for the data in dev dataset 
    prediction = clf.predict(dev_data)
    # Compare predicted labels with actual labels
    accuracy=clf.score(dev_data, dev_labels)
    print ("Predicted Nearest Neighbor accuracy for k value of "+str(k)+" is "+str(accuracy))

print("Classification report for classifier %s:\n%s\n"% (clf, classification_report(dev_labels, prediction)))

    
### STUDENT END ###

#k_values = [1, 3, 5, 7, 9]
#P2(k_values)

